# ğŸ§  Instruction-Tuned LLM using LoRA (DistilGPT-2)

An end-to-end project demonstrating **efficient fine-tuning and deployment of a Large Language Model (LLM)** using **Low-Rank Adaptation (LoRA)** and an **interactive Gradio interface**.

> ğŸ¯ **Focus**: LLM systems engineering â€” training, optimization, evaluation, and deployment  
> âŒ **Not** a ChatGPT replacement

---

## ğŸ“Œ Project Overview

Fine-tuning large language models is expensive and resource-intensive.  
This project shows how **Parameter-Efficient Fine-Tuning (PEFT)** can adapt a pretrained model to follow instructions **without retraining all parameters**.

### ğŸ”¹ What this project does
- Uses a **pretrained DistilGPT-2** language model
- Performs **instruction tuning** using the Alpaca dataset
- Applies **LoRA (PEFT)** to drastically reduce training cost
- Deploys the model via a **Gradio web interface**
- Highlights **real-world trade-offs of small LLMs**

---

## ğŸ§© Architecture Overview

User Instruction
â†“
Prompt Formatting (Instruction â†’ Response)
â†“
Base Model (DistilGPT-2, frozen)
â†“
LoRA Adapters (trainable)
â†“
Text Generation (Inference)
â†“
Gradio Web Interface


---

## ğŸ›  Tech Stack

| Category | Tools |
|-------|------|
| Language | Python |
| Model | DistilGPT-2 (82M parameters) |
| Fine-Tuning | Hugging Face Transformers |
| Efficiency | PEFT (LoRA) |
| Dataset | Alpaca (Instructionâ€“Response) |
| Training | PyTorch, Hugging Face Trainer |
| Deployment | Gradio |
| Environment | Google Colab (GPU) |

---

## ğŸ“š Dataset

### Alpaca Instruction Dataset
- **Format**: Instruction â†’ Response  
- **Samples used**: ~10,000  
- **Purpose**: Teach the model *how to respond to instructions*, not just generate text

**Example**
Instruction: Explain gradient descent.
Response: Gradient descent is an optimization algorithm...


---

## ğŸ” Why DistilGPT-2?

- Lightweight and fast
- Ideal for **educational and demo purposes**
- Clearly demonstrates **limitations of small LLMs**
- Excellent choice for showcasing **LoRA efficiency**

âš ï¸ *DistilGPT-2 is not designed for deep reasoning or factual accuracy.*

---

## âš¡ Why LoRA (Low-Rank Adaptation)?

Instead of fine-tuning **all 82M parameters**, LoRA:

- Freezes the base model
- Trains only small **low-rank adapter matrices** in attention layers

### âœ… Benefits
- ğŸš€ **10â€“20Ã— faster training**
- ğŸ’¾ **~98% fewer trainable parameters**
- ğŸ”¥ Industry-standard approach for LLM fine-tuning

**Example Training Stats**
Trainable parameters: ~1.6M
Total parameters: ~82M
Trainable %: ~1.9%


---

## ğŸ§ª Training Details

| Parameter | Value |
|---------|------|
| Epochs | 3 |
| Batch Size | 16 |
| Learning Rate | 2e-4 |
| Max Sequence Length | 256 |
| Precision | FP16 |
| Fine-Tuning Method | LoRA (PEFT) |

Training was performed on **GPU (Google Colab)**.

---

## ğŸ“Š Evaluation

The model was evaluated using:
- **Perplexity** (language modeling quality)
- **Qualitative comparison** with base DistilGPT-2
- Manual inspection of instruction-following behavior

> âœ… Priority was **engineering correctness**, not output perfection.

---

## ğŸŒ Gradio Web Demo

The LoRA-tuned model is deployed using **Gradio**, enabling:
- Live instruction input
- Adjustable decoding parameters
- Real-time text generation
- Public shareable link (Colab compatible)

### Demo Features
- Instruction text box
- Temperature & max-length controls
- GPU / CPU auto-detection

---

## â–¶ï¸ How to Run Locally

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
2ï¸âƒ£ Train the model (optional)
python train_lora.py
3ï¸âƒ£ Run CLI inference
python interface.py "Explain gradient descent in simple terms"
4ï¸âƒ£ Launch Gradio app
python app.py
ğŸ“ Repository Structure
instruction-tuned-llm-lora/
â”‚
â”œâ”€â”€ train_lora.py        # LoRA fine-tuning script
â”œâ”€â”€ interface.py         # CLI inference
â”œâ”€â”€ app.py               # Gradio web app
â”œâ”€â”€ app1.py              # Streamlit web app
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ lora_adapter/        # LoRA adapter weights only
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.safetensors
